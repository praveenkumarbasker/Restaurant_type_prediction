{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import word2vec\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences):\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return word_counts, vocabulary, vocabulary_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(\n",
    "    inp_data,\n",
    "    vocabulary_inv,\n",
    "    size_features=100,\n",
    "    mode=\"skipgram\",\n",
    "    min_word_count=4,\n",
    "    context=5,\n",
    "):\n",
    "    model_name = \"embedding\"\n",
    "    model_name = os.path.join(model_name)\n",
    "    num_workers = 25  # Number of threads to run in parallel\n",
    "    downsampling = 1e-3  # Downsample setting for frequent words\n",
    "    print(\"Training Word2Vec model...\")\n",
    "    sentences = [[vocabulary_inv[w] for w in s] for s in inp_data]\n",
    "    if mode == \"skipgram\":\n",
    "        sg = 1\n",
    "        print(\"Model: skip-gram\")\n",
    "    elif mode == \"cbow\":\n",
    "        sg = 0\n",
    "        print(\"Model: CBOW\")\n",
    "    embedding_model = word2vec.Word2Vec(\n",
    "        sentences,\n",
    "        workers=num_workers,\n",
    "        sg=sg,\n",
    "        size=size_features,\n",
    "        min_count=min_word_count,\n",
    "        window=context,\n",
    "        alpha=0.03,\n",
    "        min_alpha=0.0007,\n",
    "        sample=downsampling,\n",
    "    )\n",
    "    embedding_model.init_sims(replace=True)\n",
    "    print(\"Saving Word2Vec model {}\".format(model_name))\n",
    "    embedding_weights = np.zeros((len(vocabulary_inv), size_features))\n",
    "    for i in range(len(vocabulary_inv)):\n",
    "        word = vocabulary_inv[i]\n",
    "        if word in embedding_model:\n",
    "            embedding_weights[i] = embedding_model[word]\n",
    "        else:\n",
    "            embedding_weights[i] = np.random.uniform(\n",
    "                -0.25, 0.25, embedding_model.vector_size\n",
    "            )\n",
    "    return embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stop_words.add(\"would\")\n",
    "    translator = str.maketrans(string.punctuation, \" \" * len(string.punctuation))\n",
    "    preprocessed_sentences = []\n",
    "    for i, row in df.iterrows():\n",
    "        sent = row[\"text\"]\n",
    "        sent_nopuncts = sent.translate(translator)\n",
    "        words_list = sent_nopuncts.strip().split()\n",
    "        filtered_words = [\n",
    "            word for word in words_list if word not in stop_words and len(word) != 1\n",
    "        ]\n",
    "        preprocessed_sentences.append(\" \".join(filtered_words))\n",
    "    df = df.assign(text=preprocessed_sentences)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./\"\n",
    "df_train = pd.read_csv(data_path + \"train.csv\")\n",
    "df_test = pd.read_csv(data_path + \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.fillna(\"\")\n",
    "df_test = df_test.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"attributes.ByAppointmentOnly\"] = df_train[\n",
    "    \"attributes.ByAppointmentOnly\"\n",
    "].replace({\"b'False'\": \" \", \"b'True'\": \"yes_appointment\", \"b'None'\": \" \"})\n",
    "\n",
    "df_test[\"attributes.ByAppointmentOnly\"] = df_test[\n",
    "    \"attributes.ByAppointmentOnly\"\n",
    "].replace({\"b'False'\": \" \", \"b'True'\": \"yes_appointment\", \"b'None'\": \" \"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"attributes.RestaurantsDelivery\"] = df_train[\n",
    "    \"attributes.RestaurantsDelivery\"\n",
    "].replace({\"b'False'\": \" \", \"b'True'\": \"yes_delivery\", \"b'None'\": \" \"})\n",
    "\n",
    "df_test[\"attributes.RestaurantsDelivery\"] = df_test[\n",
    "    \"attributes.RestaurantsDelivery\"\n",
    "].replace({\"b'False'\": \" \", \"b'True'\": \"yes_delivery\", \"b'None'\": \" \"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"attributes.HappyHour\"] = df_train[\n",
    "    \"attributes.HappyHour\"\n",
    "].replace({\"b'False'\": \" \", \"b'True'\": \"happy_hour\", \"b'None'\": \" \"})\n",
    "\n",
    "df_test[\"attributes.HappyHour\"] = df_test[\n",
    "    \"attributes.HappyHour\"\n",
    "].replace({\"b'False'\": \" \", \"b'True'\": \"happy_hour\", \"b'None'\": \" \"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"attributes.GoodForDancing\"] = df_train[\n",
    "    \"attributes.GoodForDancing\"\n",
    "].replace({\"b'False'\": \" \", \"b'True'\": \"dancing\"})\n",
    "\n",
    "df_test[\"attributes.GoodForDancing\"] = df_test[\n",
    "    \"attributes.GoodForDancing\"\n",
    "].replace({\"b'False'\": \" \", \"b'True'\": \"dancing\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.assign(\n",
    "    # text = df_train[\"review\"] + df_train['city']\n",
    "    text=df_train[\"name\"]\n",
    "    + df_train[\"city\"]\n",
    "    + df_train[\"attributes.ByAppointmentOnly\"]\n",
    "    + df_train[\"attributes.RestaurantsDelivery\"] \n",
    "    + df_train[\"attributes.HappyHour\"] + df_train[\"attributes.GoodForDancing\"]\n",
    "    + df_train[\"attributes.NoiseLevel\"]\n",
    "    + df_train[\"review\"]\n",
    ")\n",
    "\n",
    "df_test = df_test.assign(\n",
    "    # text = df_test['review'] + df_test['city']\n",
    "    text=df_test[\"name\"]\n",
    "    + df_test[\"city\"]\n",
    "#     + df_test[\"attributes.ByAppointmentOnly\"]\n",
    "    + df_test[\"attributes.RestaurantsDelivery\"] \n",
    "    + df_test[\"attributes.HappyHour\"] + df_test[\"attributes.GoodForDancing\"]\n",
    "    + df_test[\"attributes.NoiseLevel\"]\n",
    "    + df_test[\"review\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[[\"label\", \"text\"]]\n",
    "df_test = df_test[[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # W2V model\n",
    "# df_train = preprocess_df(df_train)\n",
    "# df_test = preprocess_df(df_test)\n",
    "\n",
    "# tagged_data = [word_tokenize(_d) for i, _d in enumerate(df_train[\"text\"])]\n",
    "# word_counts, vocabulary, vocabulary_inv = build_vocab(tagged_data)\n",
    "# inp_data = [[vocabulary[word] for word in text] for text in tagged_data]\n",
    "# embedding_weights = get_embeddings(inp_data, vocabulary_inv)\n",
    "\n",
    "\n",
    "# tagged_train_data = [word_tokenize(_d) for i, _d in enumerate(df_train[\"text\"])]\n",
    "# tagged_test_data = [word_tokenize(_d) for i, _d in enumerate(df_test[\"text\"])]\n",
    "\n",
    "# train_vec = []\n",
    "# for doc in tagged_train_data:\n",
    "#     vec = 0\n",
    "#     for w in doc:\n",
    "#         vec += embedding_weights[vocabulary[w]]\n",
    "#     vec = vec / len(doc)\n",
    "#     train_vec.append(vec)\n",
    "\n",
    "# test_vec = []\n",
    "# for doc in tagged_test_data:\n",
    "#     vec = 0\n",
    "#     length = 0\n",
    "#     for w in doc:\n",
    "#         try:\n",
    "#             vec += embedding_weights[vocabulary[w]]\n",
    "#             length += 1\n",
    "#         except:\n",
    "#             continue\n",
    "#     vec = vec / length\n",
    "#     test_vec.append(vec)\n",
    "\n",
    "# clf = LogisticRegression(max_iter=100000000).fit(train_vec, df_train[\"label\"])\n",
    "# preds = clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This def takes in the column with the processed text from the original headlines for the training and testing sets\n",
    "# # returns two bag of words-- one for the training set and one for the testing set\n",
    "# def create_bag(df_col, test_df):\n",
    "#     # create a corpus and a generalized countvectorizer\n",
    "#     corpus = \" \".join(list(df_col))\n",
    "#     count = CountVectorizer()\n",
    "\n",
    "#     # treat the training and testing sets separately since they are 2 different dataframes\n",
    "#     docs = df_col.values.tolist()\n",
    "#     docs_test = test_df.values.tolist()\n",
    "\n",
    "#     # get two bags, but use the overall docs count\n",
    "#     bag = count.fit_transform(docs)\n",
    "#     test_bag = count.transform(docs_test)\n",
    "\n",
    "#     return bag, test_bag\n",
    "\n",
    "# # create the training and testing bags\n",
    "# train_bag, test_bag = create_bag(df_train[\"text\"], df_test[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "ps = PorterStemmer()\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# return a list of tokens\n",
    "def pre_processing_by_nltk(doc, stemming = True, need_sent = False):\n",
    "    # step 1: get sentences\n",
    "    sentences = sent_tokenize(doc)\n",
    "    # step 2: get tokens\n",
    "    tokens = []\n",
    "    for sent in sentences:\n",
    "        words = word_tokenize(sent)\n",
    "        # step 3 (optional): stemming\n",
    "        if stemming:\n",
    "            words = [ps.stem(word) for word in words]\n",
    "        if need_sent:\n",
    "            tokens.append(words)\n",
    "        else:\n",
    "            tokens += words\n",
    "    return [w.lower() for w in tokens if w not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "#TF-IDF approach\n",
    "# takes in 2 parameters as inputs and returns their respective tf-idf arrays\n",
    "def tf_idf(train_bag, test_bag):\n",
    "    # use the 3rd party library for tf-idf\n",
    "    tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=True,\n",
    "                        preprocessor=None,  # applied preprocessor in Data Cleaning\n",
    "                        use_idf=True,\n",
    "                        tokenizer=pre_processing_by_nltk,\n",
    "                        min_df = 2,\n",
    "                        max_features = 60000,\n",
    "                        norm='l2',\n",
    "                        smooth_idf=True)\n",
    "\n",
    "    # fit transform the tf-idf to training and testing bags input above \n",
    "    train_idf = (tfidf.fit_transform(train_bag)).toarray()\n",
    "    test_idf = (tfidf.transform(test_bag)).toarray()\n",
    "\n",
    "    return train_idf, test_idf\n",
    "\n",
    "# training and testing arrays\n",
    "train_idf, test_idf = tf_idf(df_train['text'], df_test['text'])\n",
    "\n",
    "#small_df = df.head(10000)\n",
    "#y = small_df.sentiment.values\n",
    "#X = tfidf.fit_transform(small_df.review)\n",
    "\n",
    "#print('bag-of-words features ready!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_df():\n",
    "    df_train[\"TF-IDF\"] = [train_idf[x] for x in range(len(df_train))]\n",
    "\n",
    "    df_test[\"TF-IDF\"] = [test_idf[x] for x in range(len(df_test))]\n",
    "    \n",
    "to_df()#add it back to df as a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_type = df_train['label']\n",
    "train_idfs = df_train[\"TF-IDF\"]\n",
    "test_idfs = df_test[\"TF-IDF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33113"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train['TF-IDF'][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33113"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test['TF-IDF'][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idfs = np.matrix(train_idfs.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idfs = np.matrix(test_idfs.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) model for tf-idf\n",
    "idf_reg = LogisticRegressionCV(random_state = 0, cv = 2, max_iter = 500).fit(\n",
    "    train_idfs, train_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_pred = idf_reg.predict(test_idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new predicted file\n",
    "dic = {\"Id\": [], \"Predicted\": []}\n",
    "for i, pred in enumerate(idf_pred):\n",
    "    dic[\"Id\"].append(i)\n",
    "    dic[\"Predicted\"].append(pred)\n",
    "\n",
    "dic_df = pd.DataFrame.from_dict(dic)\n",
    "dic_df.to_csv(data_path + \"predicted_idf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create new predicted file\n",
    "# dic = {\"Id\": [], \"Predicted\": []}\n",
    "# for i, pred in enumerate(preds):\n",
    "#     dic[\"Id\"].append(i)\n",
    "#     dic[\"Predicted\"].append(pred)\n",
    "\n",
    "# dic_df = pd.DataFrame.from_dict(dic)\n",
    "# dic_df.to_csv(data_path + \"predicted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create new predicted file\n",
    "# dic = {\"Id\": [], \"Predicted\": []}\n",
    "# for i, pred in enumerate(preds):\n",
    "#     dic[\"Id\"].append(i)\n",
    "#     dic[\"Predicted\"].append(pred)\n",
    "\n",
    "# dic_df = pd.DataFrame.from_dict(dic)\n",
    "# dic_df.to_csv(data_path + \"predicted_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_1 = pd.read_csv('predicted.csv')\n",
    "# pred_2 = pd.read_csv('predicted_2.csv')\n",
    "\n",
    "# comp_pred = pred_1.merge(pred_2, on = 'Id', suffixes=('_old', '_new'), how = 'left')\n",
    "# comp_pred.to_csv(data_path + 'comp.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(comp_pred['Predicted_old'] == comp_pred['Predicted_new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create new predicted file\n",
    "# dic = {\"Id\": [], \"Predicted\": []}\n",
    "# for i, pred in enumerate(preds):\n",
    "#     dic[\"Id\"].append(i)\n",
    "#     dic[\"Predicted\"].append(pred)\n",
    "\n",
    "# dic_df_3 = pd.DataFrame.from_dict(dic)\n",
    "# comp_pred_2 = comp_pred.merge(dic_df_3, on = 'Id', how = 'left')\n",
    "# comp_pred_2.to_csv(data_path + 'comp_2.csv', index = False)\n",
    "# #dic_df.to_csv(data_path + \"predicted_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dic_df_3.to_csv(data_path + \"predicted_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum(comp_pred['Predicted_old'] == comp_pred_2['Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum(comp_pred['Predicted_new'] == comp_pred_2['Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted_old: 0.73860 (with hyperparameter changes)\n",
    "# predicted_new (min_word increased by 1): 0.72510 -> 0.73420\n",
    "# preditect_3 (with happy_hour): 0.73420 -> 0.73770\n",
    "#predicted (added variables of noise and dancing): 0.73770 -> 0.73860"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
